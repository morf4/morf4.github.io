In the use of Hilbert's spaces, there are several specific types of linear operators
that are very important. Over the next few sections, we're going to look at these. Shortly,
we will introduce the identity operator.
Other important types of operators include inverse operators-- finding these often solves
a physical problem, mathematically, and they're also important in operator algebra-- unitary
operators, which are very useful for changing the basis for representing the vectors and
for describing the evolution of quantum mechanical systems, and Hermitian operators, which are
used to represent the measurable quantities in quantum mechanics. Hermitian operators
also have some very powerful mathematical properties.
As I said, we're going to start here by looking at the identity operator. Now, at first sight,
it might seem that this operator is of little use, operating on a function with the identity
operator does absolutely nothing of course.
In that sense, it's like multiplying a number by the number one. However, the identity operator
turns out to be very useful for some algebraic manipulations and for performing various specific
kinds of proofs. So let's see how we construct it and how it works.
The identity operator, which we typically write with a letter I, with a hat on top of
it, is the operator that, when it operates on some vector or function, leaves it unchanged.
In matrix form, the identity operator could be written as follows, with 1s all the way
down the diagonal and 0s everywhere else.
In bra-ket form, we are going to propose, here, that this identity operator can be written
in the following form. The identity operator is equal to the sum, over i, of the ket vector,
psi i, times the bra vector, psi i-- the same i, the same index in both of them.
And these psi i form a complete basis for the space. That's actually mathematically
very important. The summation here must be over a complete basis set. So let's prove
that.
We're going to take an arbitrary function, f, and expand it, as usual, with expansion
coefficients. And we know what any given expansion coefficient is. If we want to find the expansion
coefficient c m, we take the inner product of the bra vector, psi m, with the function
as a ket vector.
So if we make that substitution we can rewrite the expansion of f in this form, with these
as the expansion coefficients. Now, with our proposed form, then, I operating in f, of
course, that's just all of this operating on f.
But this entity here is just a number. That's an inner product. So we can move it about
in the product. Hence, we can rewrite this, moving this in front of the psi i, so we've
now got it here in front of the psi i ket vector. And hence, using this fact, that we
already know that the vector f is this expression, this looks the same as the expression we've
got here.
So therefore, we've just proved that I operating on f gives us f. And f was completely arbitrary.
So now we've completed our proof. We've actually proved that this form, that we had here, for
the identity operator, is, in fact, correct. Because I operating on any function f just
gives us f.
Now the statement that the identity operator I is just a sum over i, of this ket vector
for i and this bra vector for i, is trivial if psi i is the basis used to represent the
space. What do I mean by that?
Well, if we are using the psi i basis to represent these vectors, then, rather trivially, psi
1 is just the vector 1, 0, 0, 0, and so on. and therefore, the outer product of psi 1,
the ket vector with psi 1 the bra vector, the product of these two vectors, then, is
just this rather simple matrix with a 1 here and 0's everywhere else.
Similarly, if we look at the second vector, here, it would give an outer product that
looks just like this, with a 1 here and 0s everywhere else. And the third one would have
a 1 here and 0's everywhere else.
And so rather obviously, this construction here just gives us this matrix, with 1 down
the diagonals. And that's what we knew we wanted. So that's not a surprise. But if we
are talking about this same operator here and the basis we're using to write down these
vectors is not itself this basis set, then some specific psi i is not a vector with just
the i-th element being 1 and all the other elements being 0.
In fact, in general, this matrix that we would get, by taking the outer product of psi i
with itself, in general is a matrix that possibly has all of its elements non-0. But nonetheless,
the sum of all of these matrices, here, still gives us the identity matrix. That's part
of the power of this particular proof that we've just done.
So maybe these psi i are the energy eigenfunctions. But we're expressing all the vectors, for
the moment, using a momentum basis, maybe a set of plane waves for some problem that
doesn't have a uniform potential, so that the energy eigenfunctions and the momentum
eigenfunctions are not the same.
In this case, this matrix, here, would not be just a very simple matrix, with a 1 in
one diagonal element and 0's everywhere else, but still the sum of all of these matrices
would give us the identity matrix. So we can use any convenient complete basis to write
the identity matrix.
Now this expression here has rather a simple meaning when we think about in terms of vectors,
when we visualize it using our vector way of looking at things. In this expression here,
this number is just a projection of the vector f onto this axis, the psi i axis.
So multiplying this unit vector, along this psi i axis, by this number, that is this object
here-- we could write it like that if we want-- gives us, simply, the vector component of
the function f on the psi i axis. Provided that these psi i form a complete set, then
adding all of these components up again just reconstructs f.
So we are breaking f down into its vectors, the components along each of the axes here,
and then adding them all up again, so we get back to the same vector. That's what this
expression is going to do for us, this expression, here, with the f in it.
Remember that the vector in our space is the same vector, regardless of which set of coordinate
axes we choose to use to represent it. If we think about the identity operator in terms
of vectors, then the identity operator is that operator that leaves any vector unchanged.
Again, we can choose any coordinate axes we like and then project our vector out onto
those axes to give a set of component vectors. Then we can add all those components up again
to reconstruct the original vector.
That is what the identity operator is doing in the algebra we have constructed. It does
not matter which complete set of coordinate axes or, more rigorously, which complete basis
set that we use in thinking about this operation. The identity operator can be expressed in
any convenient, complete orthonormal set.
Now, you may still not be convinced that we've done anything useful here. You may still be
thinking, essentially, that all we have done is proved 1 equals 1. Perhaps you'll be more
convinced once you see the use of this identity operator in the following mathematical proof
of a rather important result.
This proof also shows the elegance of the Dirac notation. We're going to prove that
the sum of all of the diagonal elements in a matrix or a linear operator, what is sometimes
called the trace of the matrix or operator, is not changed if we change the basis used
to represent the operator.
Now let's use the identity matrix, then, in this formal proof. Since the identity matrix
is the identity matrix, no matter what complete orthonormal basis we use to represent it,
we can use the following algebraic tricks.
First, in some proof, we're going to be able simply to insert the identity matrix, in some
basis that we like, into an expression, because it doesn't do anything. Then we can rearrange
the expression and find another identity matrix buried in there. And we can take out that
result.
And, quite often, this is a very useful trick to do and enables us to conclude all sorts
of interesting things. So let's look at the proof that the trace of an operator is independent
of the basis.
Consider this sum, S, of all of the diagonal elements of an operator, A. And we're going
to be writing the operator A in some complete basis here, psi i, an orthonormal basis. So
here is what the sum of the diagonal elements are.
We remember that these were the expressions for the elements of an operator as a matrix.
And in this case, we're looking at the diagonal ones, which are the ones which have the same
index for the rows and columns.
So the sum of all the diagonal elements of the matrix is the sum, over i, of all of these
elements, which are the diagonal elements, because i is the same here on both sides.
Now suppose that we have some other our complete, orthonormal basis that we could use. So maybe
this was the energy eigenbasis and maybe this is the momentum eigenbasis for example.
We are postulating that, since this is a complete basis, we can also write the identity operator
like this if we want to. So in this summation here, let's just insert an identity operator
just before A. That makes no difference to the result, since the identity operator, operating
on A, is simply the same as A. It makes no difference to anything.
So we have, with this identity operator inserted in here and using this particular version
of the identity operator on this second basis, we can make this substitution. And then we
can start rearranging things.
So we take this expression that we had and then reorder the sums. So we have the sum
of m, inside here, and the sum of i, out here. We can flip those round. And we can move this
number, now, that is inside the summation here. We can move it from this side to that
side if we want to.
And then we can move this sum i, in here, because there's nothing out here with an i
in it. And we recognize that when we've done that, we can regroup our terms, these psi
i's here. We can regroup them.
And we recognize that this collection, here, is simply the identity operator. So we can
replace it with the identity operator-- the identity operator this time, expressed on
the basis psi i instead of the basis phi i or phi m that we were working with before.
So now, starting from here, we have now got to this point, down here. The final step is
to note that an operator, operating on the identity matrix, is just the same thing as
the operator, because the identity matrix does nothing.
And so finally, we have proved the following. Hence, the trace of an operator is the sum
of the diagonal elements. It is independent of the basis used to represent the operator.
These are the diagonal elements, when we're working on the phi m basis. Maybe those are
the momentum eigenfunctions. And these are the diagonal elements when we're working on
psi i basis. Maybe those are the energy eigenfunctions.
But the trace of the operator, the sum of the diagonal elements is the same, no matter
what basis we're working on. And we've proved that using the identity operator here, in
that neat little trick I showed you.
And of course, this fact that the trace of an operator does not depend on what basis
you express the operator on is why the trace is a useful operator property.