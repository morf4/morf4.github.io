Now we're going to look at Hermitian operators. We'll give the mathematical definition as
an equation in a moment. But in words, a Hermitian operator is one that is its own Hermitian
adjoint. That is, if we take the Hermitian adjoint of an operator that is Hermitian,
we get exactly the same operator.
This idea of a Hermitian operator is like the idea of a symmetric matrix, only we are
extending the idea to include complex matrix elements. Remember that taking the Hermitian
adjoint of a matrix involves reflecting about a 45-degree line, so flipping over on the
45 degree line, and then taking the complex conjugate.
Hermitian operators have some quite remarkable properties, and some of these are quite easy
to prove. Specifically, first, the eigenvalues of Hermitian operators are real. That's one
of the reasons why Hermitian operators are so useful for representing physically measurable
quantities. Such physically measurable quantities are, of course, real, so they should have
real eigenvalues, which, after all, are the values we will actually measure in some experiment.
Secondly, the eigenfunctions of Hermitian operators are orthogonal. Strictly, this applies
only to eigenfunctions corresponding to different eigenvalues, though there's a sense in which
it's true even for the so-called degenerate cases of more than one eigenfunction per eigenvalue.
The third very important property is that for all the Hermitian operators we will encounter
in quantum mechanics, the set of eigenfunctions is complete for the space in which the operator
operates. This is extremely useful mathematically. So let's look at what a Hermitian operator
is and show its various important properties.
A Hermitian operator is one that is equal to its own Hermitian adjoint, so simply stated
mathematically A Hermitian operator is one that has the following property-- its Hermitian
adjoint is the same as the operator itself. Note this is not the same as a unitary operator.
A unitary operator could also be Hermitian, but a unitary operator is one whose Hermitian
adjoint was equal to its inverse. A Hermitian operator is one whose Hermitian adjoint is
the same as the operator itself. Equivalently, in terminology, we might say it is self-adjoint.
In matrix terms, with some operator or some matrix M, then the Hermitian adjoint of that
matrix, of course, is just the result of reflecting about the leading diagonal here, this 45-degree
line, as it were, and taking the complex conjugates. So the Hermitian adjoint means that M31 here
turns into M13 star when we take the Hermitian adjoint.
So if a matrix is Hermitian, that is, that M is equal to its own Hermitian adjoint, then
that means that Mij is equal to Mji star. For example, here, if this matrix is to be
equal to its Hermitian adjoint, then that means that M31 has to be the same thing as
M13 star. So this matrix element has to be the complex conjugate of that one. So also,
we can conclude that in a Hermitian matrix, the diagonal elements have to be real, because
they have to be equal to their own complex conjugates.
To understand Hermiticity in the most general possible sense, consider this product here.
Our bra vector g, our matrix of interest, or our operator of interest M, and a ket vector
f. And these are going to be arbitrary vectors g and f and some operator M. And let's look
at the Hermitian adjoint of this product here.
Now, this entity here is just a number, a 1 by 1 matrix, if you like. So the Hermitian
adjoint is just the complex conjugate in this particular case. And we can also analyze this
Hermitian adjoint of this product here using the usual multiplication rule for the Hermitian
adjoint of products, that it's the flipped-round product of the Hermitian adjoint.
So this number here, this complex conjugate of this number, is equal to the Hermitian
adjoint of this same set of products. And we can write that down. So we take the flipped-round
product of the Hermitian adjoints, taking this as one entity and the g as another.
And of course, we can do the same thing again in here. We can take a Hermitian adjoint of
this product and get the flipped-round product of the Hermitian adjoints. Of course, the
Hermitian adjoint of this ket vector is just the bra vector, and the Hermitian adjoint
of this bra vector is just this ket vector. So if M is Hermitian, with therefore M dagger
equal to M, then the complex conjugate of this entity here is this one, f M g, even
if f and g are not orthogonal.
So we saw this already in terms of specific matrix elements. Maybe these were basis vectors
here and we saw this result. But this result is generally true, even if f and g are not
orthogonal. We just proved this for arbitrary f and g. And this is perhaps the most general
way in which we can write this Hermiticity condition.
In integral form, for functions f of x and g of x, then this kind of statement here we
can rewrite in the following form. And actually, what I've done here is I've taken the complex
conjugate over to the other side here, which we can always do. So the complex conjugate
of both sides, that would get rid of the complex conjugate here and put a complex conjugate
here.
So here on the left, we have just this expression-- g bra vector, M, f ket vector. And in integral
form, that would turn into this kind of expression. And on the right here, for this expression,
we would have f complex conjugate M g, and then we've taken the complex conjugate of
the right hand side, moving this complex conjugate over.
We can rewrite the right hand side in multiplying numbers and taking the complex conjugate,
since numbers commute, so there's no need to flip around the order here. And so rewriting
the right hand side, we can take the complex conjugate of this product by taking the product
of the complex conjugate. So the complex conjugate of a complex conjugate is just a function,
and we've now got a complex conjugate of this portion here.
And a simple rearrangement of that leads to this expression, that the complex conjugate
of g times M operating on f is the same thing as M operating on g and taking the complex
conjugate of that and multiplying by f. So this statement here is a restatement of this
general Hermiticity condition in bra-ket notation. So this is a common statement of Hermiticity
if we happen to be working in an integral form with functions of a variable.
Note, incidentally, that in bra-ket notation the operator can also be considered to operate
to the left. That is, this statement here, the bra vector g and the operator A, is just
as meaningful a statement as the operator A and the ket vector f. And if you think about
this as matrix vector multiplications, it's easy to see why. This is a perfectly legal
matrix vector multiplication, just as this one is.
So we can group bra-ket multiplications as we wish. So we can group this bra-ket multiplication
this way, or we can group it this way. It makes no difference.
Conventional operators in the notation used in integration, however, such as a differential
operator d/dx, don't have any meaning when we think of them operating to the left. They
don't operate to the left. They only operate to the right.
So Hermiticity in this notation is the less elegant form that we just derived, because
we don't have the option of having this operator, whatever it is, operate to the left. That
doesn't mean anything with things like differential operators that might be buried inside here.
But in the bra-ket notation, we can do that. We can operate to the left first. It's just
a matrix vector multiplication done legally. Or we could operate to the right first. It
doesn't matter.
Now we're going to look at one of the very important properties of Hermitian operators,
which is that their eigenvalues are all real. And it's quite easy to prove this. We suppose
here that we start with psi n, which is a normalized eigenvector of this Hermitian operator
M, with the eigenvalue mu n. So by definition, that means that m operating on psi n is mu
n times psi n. Therefore, if we pre-multiply by psi on both sides, then on the right hand
side we just end up with mu n times psi n psi n, which is just mu n.
But, from the Hermiticity of M, we know that this must equal this complex conjugate here.
This complex conjugate on its own, of course, is just mu n star. It's just a complex conjugate
of this. So therefore, we've proved that mu n has to equal its own complex conjugate.
So mu n must be real.
Now we're going to look at a second very important attribute of Hermitian operators, which is
that the eigenfunctions for different eigenvalues are orthogonal. So let's formally prove that.
It's quite straightforward.
We start with this rather trivial little identity here, that 0 has to equal this minus itself.
And then we decide to associate these two different terms differently. We can always
do that, because matrix vector multiplication is associative. And of course, I can if I
want to take this object and turn it into the Hermitian adjoint of the flipped-round
product of Hermitian adjoints. That does nothing.
And we can then use the Hermiticity of M to replace M here where we had M dagger up there.
M operating on psi m just gives us mu m times psi m. And similarly, M operating on psi n
just gives us mu n times psi n.
Well, these are real numbers, mu n and mu m, so we can take them outside of these expressions.
And flipping around this ket vector here, because we've got a Hermitian adjoint, turning
it back into a bra vector, then we see that what we have this mu m times psi m as a bra
vector times psi n, and the same thing here with the minus sign in front of it with the
mu n in it. So rearranging this, then, gives us mu m minus mu n times this product here.
But by definition, mu m and mu n are different. We've taken two different states here, psi
n and psi m. So therefore, 0 is equal to this inner product between these two different
functions here. Therefore, these two functions are orthogonal to one another, presuming,
of course, we're working non-zero functions.
So we've proved the eigenfunctions for different eigenvalues, that is, of mu n and mu m, are
different numbers. These eigenfunctions have to be orthogonal to one another. Since mu
m and mu n are not the same, this is not 0, so this has to be 0.
Sometimes in quantum mechanics we come across this concept of degeneracy. And it's quite
possible, especially in symmetric problems, to have more than one function associated
with a given eigenvalue. And this situation known as degeneracy. It is provable, however,
that the number of such degenerate solutions is always finite for a given finite eigenvalue.
So we will come across this degeneracy. And I've not dealt with it here because it's slightly
more subtle to deal with what happens with orthogonality in the case of degeneracy. And
we may come back to that later on. But this will come up, and it also comes up for Hermitian
operators. I thought I would just point that out here.
If a Hermitian operator satisfies a few mathematical properties, including that it is bounded,
that is, it gives a resulting vector of finite length when it operates on any finite input
vector, then the set of its eigenfunctions is complete. Certainly, for any Hermitian
operator that we believe can be approximated to an arbitrary degree of accuracy for any
calculation by a sufficiently large matrix, its set of eigenfunctions is complete for
the space that the matrix is working in. It's relatively easy to prove that the set of eigenfunctions
of a finite Hermitian matrix is complete.
When we work with matrix operations and use representations of operators with our matrices
and our computer to do some calculations, we're presuming that the operator can be approximated
to any sufficient degree of accuracy by a sufficiently large matrix. So we are used
to dealing with operators like this for physical problems. For more general cases, the proof
of this result requires setting up a mathematical framework for functional analysis that is
beyond what we can justify here, and actually beyond what is typical in quantum mechanics
courses.
Anyway, in practice in quantum mechanics, we treat the eigenfunctions of any bounded
Hermitian operator as a complete set, and we can use it to expand functions. This greatly
increases the available basis sets beyond the simple spatial or Fourier transform sets,
for example. For many problems, it means we can greatly simplify the description of it.
Indeed, as far as we know, the physically measurable quantities in quantum mechanics
can be represented by Hermitian operators with complete sets of eigenfunctions. Some
state this as an axiom of quantum mechanics.
We've already seen momentum and energy operators. The energy operator, of course, is the Hamiltonian.
We will encounter several other such operators corresponding to other physical quantities
as we get further into quantum mechanics. These operators will have the same algebra
and properties as discussed here. Hence, we have a very general, sound, and useful mathematical
methodology for discussing quantum mechanics.