At this point, we've introduced the idea of linear operators, and we've seen we can write
them as matrices. The mathematics of linear operators and matrices is very well known
and understood. There are some quite useful mathematical results for specific kinds of
linear operations, many of which simplify our quantum mechanics for us and enable some
remarkable and very general conclusions.
Now we're going to explore what these linear operators are and what they can do for us.
We're going to start here by looking at how we can write operators using the very elegant
Dirac notation. We know that we can expand functions in a basis set. For example, we
could expand the function f of x in some basis set psi n of x with expansion coefficient
cn, or we can write that in the bra-ket notation.
What is the equivalent expansion for an operator? How do we write down operators in this bra-ket
notation? We can deduce this from our matrix representation. Consider an arbitrary function
f, which we'll write as the ket f here. And we're going to be able to calculate our function
g from this, writing g as the ket here, by acting with some specific operator, A, just
as before. So, g is equal to A operating on f.
Now we can expand g and f on this basis set, psi i. So here's our expansion of our function
g with expansion coefficients di, and here's our expansion of our function f, with expansion
coefficients cj. We have a matrix representation of g equal to A operating on f, and when we
did that, we know that we got that di would be the sum over j of Aij Times cj. So Aij
is the element in the ith row of the jth column of the matrix representing A.
And by definition of the expansion coefficient here, we know that cj is just psi j, the bra
vector times f, the ket vector. So therefore, substituting for cj, we have that di is equal
to sum over j, or Aij times this inner product here. Well, we can substitute this answer
back into this expansion for g, and we'll get the following, where g is equal to the
sum over i and j of Aij times this number, and times this ket vector here.
Now, remember that this is simply a number, so we can move it about within this multiplicitive
expression. It makes no difference. Hence, we can move it to the other side of the ket
psi i, and then we can simply put brackets round about all of this part. But the g and
the f here are arbitrary, so what we're saying is that this whole object here is, in fact,
the operator A.
We had g is equal to A operating on f, while here, after we put these brackets around this
part of the expression here, we have g is equal to some operator operating on f, but
that operator must be A, so we find a way of writing out the matrix A, or the operator
A in a bra-ket form. And note that these bra and ket vectors are the other way around from
what they are in an inner product.
We have a ket vector on this side, and the bra vector on that side. This form is referred
to as a bi-linear expansion of the operator A on the basis psi i in this case. And it's
analogous to the linear expansion of a vector on a basis.
Any linear operator that operates within this space can be written this way, so we find
a rather interesting way of writing down an operator in terms of basis functions, these
bra here and ket basis functions with these numbers, which of course, are just the matrix
elements of the operator.
Now, that was in Dirac notation, and that is more general and elegant, but we should
also point that we can do the same thing in the function notation, where we're writing
something out as a function of a variable, such as x. So in that case, we might have
that g of x is equal to the integral of some operator A operating on f of x, and we have
to use a different integration variable in here, of course.
And then we can analogously write the bilinear expansion in this form, in which case we would
be saying that the operator A was the sum over i and j of Aij times the product of these
two functions, psi i of x and psi j of x1. And we need to make that distinction, the
x1 here. We can take this whole expression and substitute it in here, and of course we'll
get the right answer if we do that.
Now, what we've been looking at here, an expression of this form with the bra and ket vectors
the other way around from the inner product, this is an outer product of two vectors. An
inner product of this form here returns a single complex number. An outer product expression
of this form generates a matrix. So here's an example.
Here's the outer product of these two vectors here. Note that we have the column vector
on this side, that's the ket vector, and the row vector with the complex conjugates on
this side. That's the bra vector here. And if you multiply two vectors together this
way around, they generate this entire matrix.
So you see that this element of the matrix is this first row element here times this
first column element, and so on. You can find where all of these elements come from in this
outer product multiplication. So this specific summation here is actually then a sum of matrices.
Each one of these objects here, these outer products, is actually a matrix.
And in this matrix, a specific one, psi i as a ket vector and psi j as a bra vector,
the element in the ith row and the jth column is one. Since these are basis functions here,
presumably orthonormal basis functions. And all the other elements in this particular
matrix would actually be 0, at least if we're writing this matrix using this basis psi i.