As we showed in the last section, when the Hermitian
operators representing two physical observables commute,
they share eigenfunctions.
But what happens when the operators do not commute?
To understand this, we are going to go back and look
again at the uncertainty principle, but this time,
using the power of the linear algebra we have introduced.
We're going to be able to draw a very general conclusion
here, though it will involve a rather mathematical proof.
This proof is straightforward, but it itself does not offer
much physical insight.
However, the ultimate result here has profound and broad
physical consequences.
First, we need to set up the concepts of the mean and
variance of an expectation value.
We're going to use more of a statistical notation here.
A bar will denote the mean value of a quantity A.
So imagine, statistically speaking, we perform multiple
measurements on A. And we get an average value A bar.
Now these are going to be our quantum mechanical
measurements when we prepare the system in the same state
again and again and try to measure the quantity A. And
we'll get an average value A bar.
And in bra-ket notation, for a measurable quantity associated
with the Hermitian operator A, then when the system is in the
state f that we keep preparing it in to repeat our experiment
here, A bar will be the same thing as what we've been
calling the expectation value of A. And for a state f,
that's just this entity here.
f has a bra vector.
A is an operator.
And f is a ket vector.
So let's define a new operator that we're going to call the
delta A. So this is a mathematical operator.
It's related to the operator we had for A.
And we're going to have this operator associated with the
difference between the measured value of A in any
particular experiment and its average value.
So, that is, this new operator we're going to define, delta
A, is just the operator A minus the average value.
And to be strictly correct here, we ought to have an
identity operator in here because this is an operator.
But in a somewhat loose notation we tend to use in
this kind of algebra, we don't bother to put the identity
operator in.
So we should strictly write it this way.
But in this algebra we mostly won't bother to write that,
taking it as obvious it should be there if we've got an
expression that's basically an operator.
So we take this identity operator mostly to be
understood in our algebra.
Now, note that this new mathematical operator we've
created is also Hermitian.
A was a Hermitian operator.
A bar, which is just a number times the identity operator,
is obviously also Hermitian.
So this entire new operator here is also Hermitian.
Now, in statistics, variance is the mean squared deviation
from the average.
So to examine the variance of this quantity A that we're
going to keep on measuring--
statistically, we keep repeating our experiment,
setting up in the same state, measuring some value for A. We
get different answers in general each
time we do that possibly.
And we're looking at the variance of the
answers that we get.
So we're going to be examining the expectation value of a new
operator delta A squared.
And let's just check what this does for us.
So we're going to expand our usual arbitrary function f on
the basis of the eigenfunctions psi i of A, the
operator representing the physical quantity.
So, as usual, we perform our expansion here.
And we're going to formally evaluate the expectation value
of this operator when the system is in the state f to
check out what it does for us.
So we have the expectation value of this operator delta
A, all squared.
By definition, if we're in the state f, it's
just this form here.
And we can expand f in both cases here.
Here's our expansion of f that we wrote down.
Here's the expansion of the bra vector version of f.
Of course, we use a different index in here.
And here's our delta A squared operator written out.
Well we can operate with the first one of these, the one on
the right in this square here.
Operating on this function just gives us A j the
eigenvalue minus A the number here times psi j.
That's straightforward.
And then we can do this again.
We can operate with A minus A bar on this vector here.
We can break it into parts.
So A minus A bar operating on psi j just gives us, again, Aj
minus A bar times psi j.
So this Aj minus A bar here becomes squared now.
And, of course, because the psi i and the psi j are
members of this orthonormal set here, we only have terms
surviving in this sum over i and j when i is equal to j.
By the orthonormality of these, that gives us 1 in each
case when i and j are the same, and 0 otherwise.
And so we've just shown that the expectation value of this
operator delta A, all squared, is the sum over all the
members of the set here, with these ci squareds, the modular
squares of them, the expansion coefficients, times the
eigenvalue, minus the average.
Answer just a number, all squared here.
So this will be the average value of this squared quantity
in our statistics.
Now, because as we said these modular squareds of ci are
just a probability that the system is to be found on
measurement to be in this state psi i, then this
quantity here for that state simply represents the squared
deviation of the value of the quantity A that we are
measuring this time from its average value.
And these are the probabilities that we get
these various answers when we do this experiment here.
Then by definition, this quantity here is just this
quantity on the right.
In other words, we have indeed found that the expectation
value of this operator delta A squared is indeed the mean
squared deviation.
So we were looking for the average mean squared deviation
that we get.
So every time we do our experiment, we prepare it in
the state f.
We repeat it.
We take the answers we get for A here, the measured value.
And this operator's expectation value, or average,
here is indeed the expectation value of that mean square
deviation from the average.
In statistical language, of course, this quantity is
called the variance.
And the square root of the variance is known as the
standard deviation.
And it is written like this, obviously.
And we can call it delta A if we want to.
It's just a number here.
The square root of the average value of delta A squared.
And in statistics, of course, the standard deviation gives a
very well-defined measure of the width of some statistical
distribution.
So it's a good measure of width in the answers that we
are getting.
We can also consider some other quantity B, some other
physical quantity associated with the Hermitian operator B.
And similarly, we can write the average value of B
that we would see.
That's just the same thing as the expectation value.
And we use this form here, if we're in the state f.
And with similar definitions, we could define
a delta B all squared.
And we'd get the same answer for what that meant.
So it would also be giving us the variance of the
distribution.
So we have ways now of calculating the uncertainty in
the measurements of the quantities A and B when we do
repeated experiments, when our starting system is
in the state f.
And we can use these results in a general proof of the
uncertainty principle.
Suppose then, we've got two Hermitian operators, A and B,
that do not commute with one another.
And therefore, from our commutator, we have a
commutation rest some operator C. So in our above definition,
the commutator of A and B is equal to i times C.
So now we get into a purely mathematical proof here.
And to start that, we're going to choose some
arbitrary real number.
This could be any real number that we like.
We'll call it alpha.
And we're going to define this quantity--
this is just a number--
G of alpha.
So alpha is just a coefficient in here.
And this is what we're going to define this quantity G of
alpha to be.
So we've got alpha times delta A here, the operator minus i
times delta B operating on f.
And we're putting that whole thing in bra form.
And similarly, over here, we have it in ket form.
Now, we've written it in this form here, in ket, to
emphasize that we mean this vector.
All we're saying is that the result of alpha times delta A
minus i times delta B operating
on f is just a vector.
And this expression here is just the inner product of that
vector with itself.
So mathematically, this whole thing here is just a vector.
It's an operator operating on a vector, which gives us
therefore a vector.
So this whole thing here is the inner product of a vector
with itself.
And therefore, this must be greater than or equal to 0.
Now this is just all mathematics
here for the moment.
But this is a mathematically correct statement.
And we're writing it this way, for the moment, just to
emphasize that this thing here is just a vector, and this
thing here is just a vector.
So, as we said, it has an inner product with itself, and
that has to be greater than or equal to 0.
So, if we write this out here, as we just have from a
previous view graph, we know this has to be greater than or
equal to 0.
And now we are going to work with this here.
The Hermitian adjoint of all of this stuff here is just the
sum of these two Hermitian adjoints inside here.
So the Hermitian adjoint of i times delta B would be minus i
times delta B Hermitian adjoint.
And with the minus sign in front here, that ends up
flipping those signs around.
And by Hermiticity of these operators, A and B of course,
that means the delta A is also Hermitian.
That delta A dagger is just the same thing as delta A. And
similarly delta B dagger is just the same thing as delta
B. So we make that change here in the algebra.
And now we multiply these out.
So delta A, with an alpha in front of it, times delta A,
with an alpha in front of it, is just alpha
squared delta A squared.
And similarly, for i delta B times minus i delta B, that's
just also delta B squared.
And then we have two other terms here.
We have i delta B times alpha delta A. So
that's this term here.
We've got our minus signs here to straighten all that out.
And we have alpha delta A times minus i delta B. That's
this term here.
Well this here is just the commutator of delta A and
delta B. And we know that the commutator of these, by
definition, we're going to have that as iC here.
So that means that we can rewrite this in the following
form down here.
Therefore, this whole expression here we can rewrite
as the following, because f delta A squared f is just the
average value of delta A squared, or variance.
And we've got an alpha squared here.
And similarly, if delta B squared f is just the average
value of delta B squared, and f alpha Cf is just alpha times
the average value we would see for the quantity C. And that,
by definition, is the expectation value of Cf in the
bra form, C in the operator form, and f in
the ket form as usual.
Now, we're going to do a rearrangement of all of these
terms here.
It's not going to be an obvious rearrangement.
But it will work.
And it's quite easy to check that it does work.
So we're making this non-obvious rearrangement.
All of this set of terms here is exactly the same as this
set of terms here.
So how does that work?
Well notice here, we've written alpha plus C bar over
2 delta A squared bar.
And we squared all of that.
Well obviously, that gives us an alpha squared.
And that alpha squared is this term here, because we're
multiplying out in front.
And it also gives us a term that looks exactly like this,
but with a plus sign in front of it.
That's the square of this term here.
And because we've added that term in, we are canceling it
out by putting minus that same term over here.
And then the rest of this square here simply turns into
the alpha C bar.
And you can check that out.
We're going to get 2 times C bar over 2
delta A, all squared.
And because we've got the delta A, all squared, the bar
here out in front, that just is the equivalent
of this term here.
So we have rearranged everything here into
this form down here.
And of course, as before, this still must be greater than or
equal to 0.
All of this, as we said, is greater than or equal to 0,
must be true for arbitrary alpha.
That was one of our assumptions up front.
Alpha could be any real number that we liked.
And we could still see this statement is true.
So it's true specifically if we happen to choose alpha
equals minus C bar over 2 delta A squared bar.
And that makes this term here just be 0.
So therefore, what we're left with, from all of these terms
here, we can rearrange them very slightly.
We're left with delta A squared bar times delta B
squared bar is greater than or equal to C bar
squared, all over 4.
Or equivalently, delta A times delta B is greater than or
equal to mod C bar over 2.
So for two operators A and B corresponding to measurable
quantities A and B, and for which the commutator of A and
B is equal to i times some operator C, in some state f
for which C bar-- that's the expectation of C. It's written
out like this, of course--
we have the uncertainty principle that delta A times
delta B is greater than or equal to the modulus of C bar
all over 2.
This is our general statement of the uncertainty principle.
And here delta A and delta B are the standard deviations of
the values of A and B that we would measure.
So this uncertainty principle is quite general for any
operators A and B, where we can evaluate their commutator.
And we see, therefore, that we get an uncertainty principle
if the operators do not commute.
And we've managed to prove this without presuming
Gaussians or any other specific form for
distributions.
This is generally true.
Only if the operators A and B commute, that is, A and B have
a commutator 0, or strictly speaking 0
times an identity operator.
Or possibly if they do not commute, they have a
commutator rest.
But we're in a state f for which this expectation value
of the commutator rest is equal to 0.
Only in those cases is it possible for both A and B
simultaneously to have exact measurable values.
So we have a very general result.
Whether or not the operators associated with two physically
measurable quantities commute with one another determines
whether or not there's an uncertainty
principle between them.
This is a core concept in quantum mechanics.
Indeed, one can take the view that it's this absence of
commutation between such operators that is the real
mathematical distinction between the classical and
quantum worlds.
In the mathematics of classical mechanics,
attributes like position and momentum and energy and
angular momentum are represented by numbers, or
ordinary vectors.
The multiplication of numbers commutes, as does the dot
product of vectors.
But for the quantum mechanical view, we have introduced
operators for these attributes, and the
multiplication of these does not in general commute.
Whenever that computation is absent, we have an uncertainty
principle for those attributes, a very
non-classical idea, at least for those particular
attributes.
Some people even take the view that it's the postulation of
these so-called commutation relations that's the real core
set of postulates of quantum mechanics.
That this is the real difference between the
classical and quantum descriptions.