Matrix notation is basically a way of writing a rectangular array of numbers. So an n-by-n
matrix would have n rows -- here, two rows. This is a row, so there are two of them. And
rows are horizontal. And n columns. So here, there are three columns. So n is 3, and the
columns are vertical.
And conventionally in matrix notation, this array is enclosed in these square brackets.
And this here would be described as a two by three rectangular matrix. So two rows,
and three columns. And we always specify matrices in this order -- rows, then columns.
The symbol for a matrix could just be a capital letter, for example, like A, and that's very
common. Actually, here in the quantum mechanics I'm going to do, I need to be able to distinguish
matrices and other so-called linear operators from numbers and simple variables. So I'm
going to adopt a notation where I put a hat over here, over the symbol A, to be very clear
that I'm talking about a matrix or an operator. So I need to make that distinction, so I use
this notation, which is also quite common in quantum mechanics, but relatively unusual
in just ordinary matrix algebra.
But wherever I have a symbol A with a hat over it, that's something that can be thought
of as a matrix, or more generally is what we will call a linear operator. So I'll try
to be clear about that notation. So I'm always putting a hat over the symbol representing
a matrix.
And another convenience of that is that when you're writing these things down by hand,
putting this hat over here by hand clearly distinguishes it from some other possible
notation you might use, like using characters without serifs on them, sans serif characters.
Like, these are sans serif, but that's a serif character, or using bold characters. It's
very difficult to write those and make them clear by hand. But this hat does clarify that
this thing is a matrix or an operator.
Because all matrices are, by definition, rectangular, when we say a matrix is rectangular, we are
usually meaning that it's not a square matrix. A square matrix is one which obviously has
equal numbers of rows and columns, and this is a square matrix here. It's a two by two
square matrix.
The numbers are what we call elements in a matrix. They can be real numbers, imaginary
numbers, or complex numbers. The elements are always indexed in row-column order. So
just as this is the number of rows and this is the number of columns, when we write the
elements of a matrix, we'll use subscripts when we're referring to a specific one.
And this would be the element in the first row and the second column. So here, first
row, second column is this element here. And this has value minus 0.5i in this case.
And we will often use the same letter B for the matrix and for its elements. So this is
the matrix B. And I've used the same letter here when I've been talking about the elements.
There's no requirement to do that. It's just common.
And sometimes, instead of using the uppercase letter, which we might use for the matrix,
we might use a lowercase letter for the element. As I say, this is not required notation. It's
just something you commonly see.
Now the leading diagonal of a matrix, or just a diagonal, is the diagonal from the top left
to the bottom right. So it's this diagonal set of numbers here. And elements on the diagonal
are rather obviously called diagonal elements.
So these two elements, 1.5 -- they're both the same in this case -- are the diagonal
elements of this matrix. And rather obviously, the ones that are not on the diagonal are
called the off-diagonal elements. So these elements, 0.5i and minus 0.5i, are the off-diagonal
elements.
The particular case of a matrix that has only one row or one column is called a vector.
Now on the face of it, this seems to be a completely different use of the term vector
from the idea of geometrical vectors. And it is true it has some differences.
However, we're going to give these new vectors many mathematical properties that are quite
analogous to the properties we have in geometrical vectors, such as, for example, ideas of orthogonality,
which in geometrical vectors corresponds to the right angles, and also length. And this
will make our new vectors a generalization of the concept of a vector as used in geometry.
A geometrical vector can be represented by a list of three numbers -- these being its
components along three coordinate axes.
In our generalized form of a vector, it can have any number of elements. Technically we
might say that our generalized form of a vector can have any number of dimensions, not just
the three dimensions of ordinary geometrical space. And another important extension is
that these elements of our mathematical vector now may also be complex numbers, in contrast
to geometrical vector components, which are always real. We always have a real number
projection along axes.
It is also true, though, that we can use our new idea of vectors and their algebra to represent
geometrical vectors and much of their algebra, such as addition and dot products. So it is
meaningful to think of these two kinds of vectors as being versions of the same idea.
So for our vectors, in the matrix algebra version of vectors, they are simply matrices
of size 1 in one of the directions. And we have to specify when we're talking about a
vector, whether it's a row vector. So this is a row vector. It's a matrix with one row,
or a vector -- a row vector.
Or a column vector. So here's a column vector. It's a matrix with one column.
Technically, a vector is a matrix. But we almost exclusively use the term "vectors"
for such single row or single column matrices. Because of the way we use such vectors in
linear algebra generally, we tend to think of them differently, and we use a different
set of symbols for them from those that we use for matrices. In physical problems, for
example, vectors typically represent the physical state of the system. So the vector could be
representing the wave that we currently have, or perhaps the density in space of something
or another.
But the matrices often represent some manipulation we're going to perform on a system. For example,
in the classical world, we could have a vector, such as velocity, with velocity being an example
of the state of a system. Our matrix could then represent some change we're going to
make, or mathematically, some operation we're going to perform, such as changing the direction
and magnitude of the velocity in some way.
It is, of course, possible that a matrix representing some operation just happens to be a one by
n matrix. But in thinking about a physical problem, we would still be making a conceptual
distinction between that matrix, which happens to be one by n, and an n dimensional vector,
even though all the algebraic properties of the two are the same.
A very common operation we have to perform on matrices and vectors is called the transpose.
And it's denoted by a superscript, T, and it's a reflection about a diagonal line from
the top left to the bottom right. So the result of this operation is that the 6 is going to
move up to this position, the one will move down to that position, the minus 5 will move
up to this position, and the minus 3 will move to this position. The 2 and the 4 still
remain at the top left and bottom right of the matrix.
So algebraically, in terms of the elements of the matrix, the mn-th element of the transpose
of a matrix -- that's the element in the m-th row and the n-th column -- is the same as
the element in the n-th row and the m-th column of the original matrix.
Here's another example. If we take this matrix here, which happens to be a square matrix.
Again, we reflect on a diagonal line here, from the top left to the bottom right, and
we obtain the transpose.
And in this particular case, all that has really happened is that this element and this
element have been swapped over. And algebraically, again the same thing, the mn-th element of
the transpose is the same as the nm-th element of the original matrix.
We can do the same for vectors, and we could use the same idea here, but it's a little
more difficult to see what that line would look like in a vector. So instead, we might
think in a vector case that we're using a 45 degree line. So here's our row vector here.
We're putting a 45 degree line here on our picture. I'm going to reflect about that 45
degree line to get the column vector version.
And here's another example. In this case, we start out with a column vector -- a complex
one in this case. All the elements happen to be complex. We reflect about our 45 degree
line, and we get a row vector here. Sometimes, this notation is used because it just takes
up too much space to write the column vector. More often than not in physics, like quantum
mechanics, we're actually working with column vectors nearly all the time, but it can be
very space-consuming to write out column vectors, so it's quite a common notation just to write
the vector in its transposed formed just for notation's sake.
Now a related operation, but one that is a little bit different, is the Hermitian transpose
or adjoint. There are various different names for this. It can be called the Hermitian adjoint,
or the Hermitian transpose, or the conjugate transpose. And incidentally, this is named
from the name of the French mathematician, Charles Hermite.
But unfortunately, the name Hermite is not so obviously French, and in the English language,
people mostly mispronounce it was "hair-mite." So it should be Hermite, but it's usually
pronounced "hair-mite." And we tend to pronounce the H here, so we call this Hermitian adjoint,
Hermitian transpose.
And I apologize to the French for that.
Also, it's sometimes known as the conjugate transpose. And this same name, incidentally,
will come up later, because he is well known also for coming up with a set of polynomials
which are usually known in the English language as the "hair-mite" polynomials, but they should
be the Hermite polynomials.
The Hermitian adjoint, or the Hermitian transpose, or this conjugate transpose, is denoted by
a superscript. And the character in here is what's known as a dagger, like a knife with
a handle on it. And it corresponds to taking a reflection about the same line from top
left to bottom right, and very importantly, taking the complex conjugate of the elements.
So here, I've reflected in this line so the 0.3i has gone up to this position, but we've
also had to take the complex conjugate. So that has turned 0.3i into minus 0.3i. And
similarly, the minus 0.5i here up in the top right corner has become plus 0.5i here in
the bottom left. And if it happened that the diagonal elements were complex or imaginary,
we would also see that we're taking the complex conjugate of them, although that's not obvious
in this specific example. And when we write that out in the subscript notation, the mn-th
element of the Hermitian adjoint is the nm-th element of the original matrix complex conjugated.
Similarly for a vector here, we're going to do the same kind of thing, although we will
probably think of this as having a 45 degree line to do the reflection for the vector,
just as we did for the transpose. And you see here that rather explicitly, we've ended
up taking the complex conjugate of all of these different elements. The signs of all
of the imaginary parts have changed when we've gone into this Hermitian adjoint, Hermitian
transpose, or conjugate transpose of the vector.
A very special and important kind of matrix is one that's called a Hermitian matrix. And
a matrix is said to be Hermitian if it is equal to its own Hermitian adjoint, or its
own Hermitian transpose, or conjugate transpose. So that means that if a matrix is Hermitian,
B dagger -- the Hermitian adjoint -- is equal to B.
So here's an example of a matrix that is Hermitian. The values of the off-diagonal elements here
are 0.5i and minus 0.5i. So when we reflect in the diagonal, we take this element up to
here and this one down to there. And in the course of doing that, we also take the complex
conjugate, but the complex conjugate of 0.5i is minus 0.5i. So this specific matrix has
its Hermitian adjoint equal to the original matrix.
And element by element, that means that the mn-th element of B dagger is the same as the
nm-th element of B. If two matrices are the same for all of their elements, then the matrices
are the same. And note that the order here of these subscripts is the same on both sides.