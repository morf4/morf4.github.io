For a very broad class of the kinds of functions that we come across in physics, we can presume
that they are what is called analytic, except possibly at some singularities, or at least
within some region. So at least within some range of values of our argument x, maybe near
some point x-naught, we're presuming that our function f of x can be approximated arbitrarily
well by a power series.
For example, if we are looking around about the point where we're seeing this x-naught
is 0, then our power series is a-naught plus a1 x plus a2 x squared and so on. For example,
if the function was actually a parabola, we would just need the first three terms in this
series with some numbers for a-naught, a1, and a2. But possibly, this series goes on
and on and on. Maybe it's infinite. Incidentally, we are using as a notation here that we may
have come across before called an ellipsis. And that simply means that we are omitting
writing down some of the terms explicitly.
Also, if we took a more general point, rather than saying that x-naught was equal to 0,
we are saying that round about some other point, this function f of x we are presuming
can be written in this kind of a form where now our powers are x minus x-naught that we're
raising to a power.
Taking the simplest case, where we are going to take x-naught equal to 0 first, we are
presuming we can write the power series like this. So let's figure out what these various
numbers are, or how to deduce what they are from what else we know about the function.
So obviously, if we take x equal to 0, all of these terms completely disappear. So we're
just left with a-naught. So we know, therefore, what a-naught is. A-naught is f of 0.
What about a1? Can we figure out what a1 is from a knowledge of the function? Well, if
we take the derivative of f of x with respect to x, then the derivative of a-naught itself
is just 0. The derivative of this term here is a1. And the derivative of this term here
is 2 a2 times x, the derivative of this term here is 3 a3 times x squared, and so on.
So at the point x equals 0, these terms all disappear, and we're left just with this term.
And so we've now found a way of figuring out what this coefficient is. So a1 is the derivative
of f at the point 0.
Now, remembering this here, what we're going to go on and do next is find the second derivative.
So we're going to be taking the derivative of this expression. So here is that derivative,
the second derivative now. And we had 2 a2 x when we just looked at the derivative itself.
And taking the derivative of that to get our second derivative, we have 2 a2 and then a
bunch of other terms here. We can also write these, incidentally, using the factorial function.
So again, at the point x equals 0, all of these terms disappear. These ones out here
all disappear. And we're just left with this term here.
And we can carry on like this. If we take the third derivative, we're taking the derivative
of all of this expression. This term will disappear because it's just a constant. And
we will end up having this term turned into simply 3 factorial a3 and so on. We could
keep on going like this.
So continuing with this process, we can deduce what the values of all of these various coefficients
are. So this was a-naught. All of this here is a1 times x. All of this here is a2 times
x squared, and so on. And all of this here is an times x to the power n and so on.
So what we are saying is that if we know the function and can evaluate all its derivatives
near to a point here -- the point x equals 0, then we can also write the function out
as a power series. Now, we have to be somewhat careful with this. If we choose x to be too
large, it's possible that this power series may not converge. So we always have to be
careful with a power series that it may only be valid for some range of x. But nonetheless,
we can write out power series usefully for many functions. And sometimes, the so-called
range of convergence is not constrained. It may be very large, or possibly even infinite.
Now, instead of doing this process round about x equals 0, if we did it instead round about
some other point, x equals x-naught, then with our presumed power series that we are
trying to find these coefficients for, we can, in fact, proceed in exactly the same
kind of way, and we will get what's called the Taylor series. And this here is the Taylor
Series. It's essentially the same as the Maclaurin series, but instead of everywhere we have
x here, down here we have x minus x-naught. So it's x minus x-naught raised to all these
powers.
So let's look at some example power series expansions of common functions. The function
1 over 1 minus x, If we carefully work that out, we will find that that one is 1 plus
x plus x squared plus x cubed and so on. Again, this will only work for some range of x. But
for small x, this will work fine. The exponential of x is 1 plus x plus x squared over 2 factorial
plus x cubed over 3 factorial, and so on for its power series.
The power series for the cosine of x is 1 minus x squared over 2 factorial plus x to
the fourth over 4 factorial. And the power series for sine of x is x minus x cubed over
3 factorial plus x to the fifth over 5 factorial minus the next term and so on. So note that
the signs of these terms are alternating in the cosine and sine expansions. And if you
check it out, you can, in fact, use Euler's formula to deduce these power series from
this one here.
Now, the other very important point about power series, especially in physics and in
modeling systems of various kinds, is the idea of using power series for approximate
results. So Maclaurin and Taylor series allow us to make approximations for small ranges
round about some specific point.
So here are some examples for small x for some functions. And these are the approximations
which are valid to so-called first order in x. And it's another important point about
this whole notion here is that it helps us establish what we call the order of an approximation.
So a first-order approximation to 1 over 1 plus x, from our power series that we were
looking at before, we can evaluate the power series of 1 over 1 plus x. And that is 1 minus
x to first order. So as you start to increase x from 0, the first thing you will see is
that this function corresponds to something that starts out decreasing linearly in proportion
to x. So the first-order approximation to this function here is one that shows a linear
decrease with x, just to start with.
Similarly, this expression is one that comes out quite often, and we like to know an approximation
for it for small x. And the answer to that is 1 plus x over 2. For the exponential function,
again, the small x approximation to that is 1 plus x, the power series expansion first
term to the power x to the power 1. And the logarithm of 1 plus x, which is related to
this here, is just x to the lowest order, to first order in x. Sine x to lowest order
in x is just x. Tan x is just x. So all of these are what we would call first-order approximations
to the functions.
Note, incidentally, that cos of x is an example of something where the lowest order of approximation,
at least the one that depends on x, is not first-order. The lowest nonzero order is actually
second-order. So the lowest-order approximation to cosine of x that actually depends upon
x is 1 minus x squared over 2.