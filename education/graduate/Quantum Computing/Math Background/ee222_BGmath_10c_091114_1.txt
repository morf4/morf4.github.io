The solution of some matrix equations can show a very useful phenomenon, which is that
they may have eigenvectors and eigenvalues The essence of an eigenvector is that when
we multiply this eigenvector by our matrix, we're going to get back the same vector, possibly
multiplied by a number. And that number is called the eigenvalue.
If we think by analogy with ordinary geometrical vectors, when a matrix multiplies one of its
own eigenvectors, the result is a vector that we can think of as pointing in the same direction,
but with a length multiplied by the eigenvalue. The word eigen comes from German, where, loosely
speaking, it means own or self. In fact, it is etymologically related to the English word
own, eigen and own.
For an eigenvector of a given matrix, when we multiply it by that matrix, we get back
the self same vector, though possibly of a different length. So this is where the idea
of self comes in with eigenvectors. The behavior of eigenvectors and eigenvalues of matrices
is found in a generalized form in quantum mechanics, where we may have eigenfunctions
instead of eigenvectors. Though, in a deeper sense, the ideas of what we call eigenfunctions
and what we call eigenvectors are really the same thing. And we will, in the end, be able
to use the matrix vector form, even when we are dealing with eigenfunctions instead of
what we might think of as eigenvectors.
Hence, the essence of the ideas of eigenvectors and eigenvalues with matrices is particularly
important in the mathematics we need for quantum mechanics. So let's look at a matrix eigenequation.
An equation of this form, a matrix A times a vector d equal to a number lambda times
the same vector, for a square matrix, is called an eigenequation.
So this lambda is called the eigenvalue. This d is called an eigenvector. And if there are
solutions of this equation, and that's not necessarily the case, they may only exist
for specific values of lambda. So if this equation has a solution, which it might not,
if there are solutions, they may exist only for specific values of lamda, for specific
eigenvalues.
So let's look explicitly at how we're going to solve an eigenequation for matrices. So
we start out with our eigenequation. And then we rewrite it, rather trivially, by putting
in the identity operator here. We could always do that because the identity operator does
nothing. I times d, this vector, is just d. So we can always put an I in there if we want
to.
And so we can rewrite this equation. We can take this lambda I over to the left. So we've
got this now just a matrix A minus lambda I times d is equal to zero. So all we're doing
here is rewriting our matrix. We have to be a little careful with the notation here. This
zero here is not the number zero. It's actually a vector with elements that are all zero.
And this notation is somewhat loose. We should have been clearer that this is actually a
vector with elements zero and not just a number.
But once we've sorted that out, we realize that what we've done is we've rewritten this
equation up here in this form. It's exactly the same equation, but now it's got a different
character to it. It's saying, some matrix B, multiplying this vector, is going to give
us a zero vector as a result. Now why did we do that?
Well, we know something about this. If some matrix B times a vector is to give us a zero
vector as a result-- and the vector d itself is not going to be zero vector. That would
be trivial-- that means this matrix B cannot have an inverse. If it did have an inverse,
which we could write as B to the minus 1, then we can multiply both sides of this equation
here. B to the minus 1 times B is, of course, just I, on the presumption that this inverse
exists. So that means B to the minus 1 times B times the vector d is just d.
And that must be, of course, looking at the other side of the equation, B to the minus
1 operating on this zero vector, the vector with elements all zero. But any finite matrix
multiplying a zero vector must give a zero vector. So there is no non-zero solution d.
There's no vector d that isn't already just all zeros that can solve this equation here
if this matrix B has an inverse.
So by what we call reductio ad aburdum, B has no inverse. If it did have an inverse,
we'd get to a silly situation here which can't exist. So therefore, B cannot have an inverse.
That's a reductio ad absurdum proof. So if we have this equation here, which was just
a rewrite of our original eigenequation, for this equation to have any solutions, it must
be the case that the matrix B has no inverse.
Now if the matrix B, which, remember, is just our original matrix minus the eigenvalue times
the identity matrix, if that matrix B has no inverse, it means, from the properties
of our determinant, that the determinant of that matrix has to be zero. Remember, we said
that before. If the determinant of a matrix is zero, the matrix has no inverse. And if
the matrix has no inverse, the determinant is zero. That's one of the magic properties
of determinants.
This equation here will allow us to construct what's sometimes called the secular equation.
And the solutions of that secular equation will give us the values lambda for which our
eigenequation is going to have a solution. And once we figured out those eigenvalues
lambda, the values of lambda for which our eigenequation has a solution, it's relatively
straightforward to deduce what these eigenvectors are.
So suppose we want to find the eigenvalues and eigenvectors, if they exist, of a two
by two matrix like this. It's a square matrix. It happens to be a complex one. Imagine any
elements on the off diagonal. So we write the determinant condition for finding eigenvalues.
The determinant of this matrix has to be zero. That means the determinant of all of this
matrix added up in here has to be zero.
Now, the determinant of all of this stuff here, we can add everything together. We take
our lambda and actually multiply the identity matrix by it. So that's this matrix lambda
lambda here. We then add these two matrices with the minus sign in here. And we get this
matrix. So our equation, now, is we want the determinant of all of this matrix to equal
zero.
So our secular equation becomes, the determinant equals zero here, gives us all of this here.
That is, the determinant was this term times this term minus this term times this term.
So here we are. That's this term times this term. And minus this term times this term
is that there. We've got to watch our minus signs carefully here because we get an I squared
in the middle.
But what that turns into is 1.5 minus lambda all squared minus a quarter. That's what we
get from multiplying these two minus 0.5's with the additional I squared in here. It
has to be zero. Or, equivalently, we can multiply this out. And what we've got is a quadratic
lambda squared minus 3 lambda plus 2 has to equal zero.
Well, of course, we can solve that quadratic equation. That's straightforward enough. And
if we do so, we'll get these roots here. So lambda 1 equals 1. Or another root is lambda
2, we'll call it, equals 2. These are the eigenvalues. These are the values of that
parameter lambda for which there will be solutions of our original eigenvalue equation. So we
substitute these values back into our original eigenvalue equation and deduce the corresponding
eigenvectors.
Let's just see that working. So our original eigenvalue equation was this one. And we've
now figured out what values of lambda will allow this equation to have a solution. So
for a given eigenvalue that we choose here, we're trying to find out, then, what will
be the eigenvector, that is, what will be the values of d1 and d2 here.
Rewriting this equation here, we can simply take this over to the left hand side. So 1.5
minus lambda, that's having taken that over on the diagonal here, minus 0.5i 0.5i, as
before, and then, again, we've got minus lambda d2 to put on the diagonal here.
So we've taken all the stuff on the right and pushed it into the matrix on the left.
We're left with a zero vector on the right. We can evaluate this now for a specific eigenvalue.
So let's take, say, the first eigenvalue, lambda 1 equals 1, that we just figured out.
So putting lambda 1 equal to 1 into this matrix here gives us this matrix equation.
And now, conveniently, we can actually go back to the writing out the matrix equation
as a couple of linear equations. So this matrix equation here is the same thing as these two
linear equations here. And from either one of these equations, it doesn't matter which
one-- this was the first of them. This is the second of them-- we can now deduce the
eigenvector. Because either one of these equations simply gives us that d2 is equal to i d1.
So we could take this equation here. We'd have 0.5 d1 is equal to 0.5i d2 over here.
In other words, d1 is equal to i d2. And you get the same answer if you took the second
equation.
Now, with eigenvectors, we are free to choose any one of the elements here. It doesn't really
matter what we choose for one of them. All we have is a relation between the two of them.
So let's choose, say, d1 is equal to 1, which gives us the eigenvector v1 here, v1 equal
to 1i. And similarly, if we went through the same process using lambda 2 equal to 2, we
would get the other eigenvector, which would be 1 minus i.
For larger matrices with eigensolutions, that is, for some n by n matrices here, we would
have correspondingly higher order polynomial secular equations. And those might get hard
to solve. But they, in principle, might have solutions. And they would then give us n eigenvalues
and eigenvectors if those eigenvalues and eigenvectors exist, if the solutions exist.
So a three by three matrix can have three eigenvalues and three corresponding eigenvectors.
Note, incidentally, as I said before, when we had the freedom to fix one of our elements
in our two by two matrix eigensolution, eigenvectors can always be multiplied by any constant and
still be eigenvectors. It doesn't matter.