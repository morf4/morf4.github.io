Because of the mathematical equivalence of matrices and linear operators, the algebra
for linear operators is identical to that of matrices. In particular, these linear operators
do not, in general, commute. What do we mean by that? We mean that A after B operating
on f is not, in general, equal to B after A operating on f, for some arbitrary f.
And you may remember from matrix multiplication that it's not in general true that the matrix
A times the matrix B is equal to the matrix B times the matrix A. Sometimes it is. Sometimes
it's not, but you certainly cannot rely on that.
And whether or not operators commute turns out to be very important in quantum mechanics,
and has some real, physical meaning to it. So sometimes our operators will commute. In
general, they do not. And matrix algebra tells us you cannot expect, necessarily, that matrix
multiplication-- and hence, linear operator multiplication-- is commutative.
Now so far, we discussed operators for the case of functions of position, such as position
x. And we used this argument to show that the matrix was the best possible, most general
way of writing a linear operator, at least when we were talking about position. But we
can also use the same argument to deduce that this also works when we're talking about operators
expressed as expansions on basis sets.
So we had expanded the function f of x, for example, in some basis here, with the expansion
coefficients cn. And we could also expand the function g of x, similarly, with expansion
coefficients dn. And as I said, we could have followed a similar argument requiring each
expansion coefficient, di, to depend linearly on all of the expansion coefficients, cn.
It would've been exactly similar argument to the one that we just went through.
So therefore, by these similar arguments, we would similarly deduce that the most general
linear relation between the vectors of expansion coefficients could be represented as a matrix.
And that matrix, in general, would look like this. So what we're seeing is that the most
general possible linear relation here, whereby, for example, this element d1-- the expansion
coefficient d1 for our function g-- the most general way that could be related to the expansion
coefficients for our function f would be through some series that we would get by multiplying
c1 times a11, c2 times a12, and so on. And similarly for of the other expansion coefficients
of our function g.
So this matrix vector multiplication would be the most general possible way in which
we could get a linear operation in which we deduced all these expansion coefficients,
or function g, from all these expansion coefficients, or function f. And the bra-ket statement of
the relation between f and g and the operator, A, remains unchanged. It's the same bra-ket
statement if we write these functions, f, out as this column vector here, in terms of
expansion coefficients, and we write g out as this column vector here, in terms of its
expansion coefficients. Remember, the notion is that just because we now are writing the
functions out as expansions on a basis, the vector itself is still the same vector in
our Hilbert space.
Now we're going to find out how we can write some operator as a matrix. That is, we will
deduce how to calculate all the elements of the matrix if we know the operator. Suppose,
then, we choose our function f of x to be one specific one of the basis functions. We'll
say psi g. So that f of x here is this function psi g of x. Or equivalently, in bra-ket notation,
the ket, f, is the same thing as the ket psi j.
Then in this expansion for f of x in terms of the psi n basis set, we're choosing cj,
some specific one, equal to one. And all the other cs are going to be zero. Now we operate
on this function with the operator. In this kind of expression, we're generating a new
ket vector, g, from the ket vector f by operating on f with the operator A. And suppose specifically
that in this ket vector, g, we want to know this specific expansion coefficient, di. So
from the matrix form of this bra-ket version g equals A operating on f, then we know that
our matrix form looks like this. And with our choice, cg equals 1 and all the other
cs 0, then we're going to have that di is equal to Aij.
Let me illustrate that just by example. Suppose that we choose j equals 2. So that is c2 equals
1. And all the other cs are 0.
Then our vector for f would be one in which this element is 1-- that's c2-- and all the
others are zero. And then if we multiply this vector into this matrix here, then, specifically,
d3 will be the result of multiplying this vector into the third row of the matrix. And
this is 1, and all the others are 0. So the result of that multiplication of this vector
and this row here is just this one element. So d3 in here would just be a3 2-- the two
from here, and the 3 from our choice of which d we wanted.
But from our expansions for f and g, for the specific case where f is just psi j, then
g is equal to, of course, its expansion-- dn psi n. And that's A times f. And in this
specific case, because f is psi j, that's A times psi j.
To extract di from this expression here, all we need to do is pre-multiply on the left
by the bra vector psi i. If we operate with psi i in here, all that's going to happen
is we're just going to extract di from this sum. And also on the right, pre-multiplying
by psi i in the front here, that means therefore that di is equal to psi i A psi j.
But we already concluded for this case that di was equal to Aij. So therefore, Aij, this
matrix element, is psi i A psi j. So we've figured out how to evaluate this matrix element
of this operator on this psi basis.
Our choices of i and j here, of course, were quite arbitrary. So quite generally, when
writing an operator, A, as a matrix, we can use this same technique for every single matrix
element. When using a basis set psi n, the matrix element of this operator are Aij is
psi i A psi j. For example, we can now use this to turn any linear operator into a matrix.
And the simple, one-dimensional spatial case would give us this actual expression for what
Aij is.
This is the same thing as the bra-ket version up here, just written out now as an integral,
if we happen to have a one-dimensional spatial function. Incidentally, note that this operator
in here could be something like the Hamiltonian. It could be a differential operator. A Hamiltonian's
got differential operators in it. And we can perform this integral.
We just have this differential operator operate on the wavefunction, if that's what it is.
We get a new function as a result of that, and we evaluate this integral. So we can turn
all sort of operators-- differential operators, any linear operator-- into a matrix with these
matrix elements.
Now let's look at a visualization of what's really going on in a matrix element. The operator
A is going to act on this unit vector psi j here. And that's going to generate a new
vector. The vector is A operating on psi j. That gives a specific vector as an answer.
And in general, what has happened is the action of A operating on this vector has rotated
it to a new direction and given it a new length. There's actually really nothing else that
an operator can do, at least in this geometrical analogy here. The matrix element, psi i A
psi j is the projection of this vector onto this axis. So explicitly, it's this length
here that is this object.
Now we're using a geometrical analogy here, and the only weakness of that is we can't
really indicate the complex values that are going on in these various projections. But
the concept is basically this kind of geometrical concept. So this length here, as a complex
number, is what the matrix element is.
Note that, in general, all that a linear operator does in our visualization is to take some
vector, like this vector here, and rotate it some way or other, and compress it or stretch
it. The components along each axis, or basis function, change as we do that. This analogy
is a little incomplete in that these components or projections are, in general, complex numbers,
in our case. But this visualization can still help us think about what linear operators
are doing to vectors in Hilbert space.
So just to reiterate that, here's our vector here. We operate on it with an operator. What
that does is rotates it and maybe changes the length of it. That's what an operator
does to a vector.
So then, we can write the matrix for the operator A as this expression here, where we've written
out explicitly in terms of the basis functions and this object psi 1 A psi 1, for example,
or some other one down here, psi 3 A psi 2. We've written out the matrix explicitly in
terms of all of these objects. This is how we evaluate the matrix for an operator when
we want to work on some specific basis here-- this psi basis, whatever it is.
So overall, we've now deduced how to set up a function as a vector, and a linear operator
as a matrix. And this matrix can operate on the vectors. So now we've completed our basic
setup of how we're going to look at the linear algebra in quantum mechanics.
Quantum mechanical states will be represented by vectors-- either column vectors, which
become these ket vectors in the Dirac notation, or row vectors, which become the bra vectors
in the Dirac notation. Quantum mechanical linear operators will be represented by matrices.
The effect of matrices and vectors, and the effect of quantum mechanical operators on
quantum mechanical states, is to stretch or compress them, and rotate them in Hilbert
space, subject only to the sophistications that our space may have any number of dimensions,
depending on the problem we're considering, and the components of the vectors are generally
complex numbers.